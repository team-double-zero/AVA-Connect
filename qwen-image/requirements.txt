# GPU 서버면 cu121 권장, CPU 서버면 그냥 "torch"로 바꿔도 됩니다.
torch --index-url https://download.pytorch.org/whl/cu121

# 바이너리 충돌 방지를 위해 numpy 고정
numpy==1.26.4
pillow
accelerate

# Qwen-Image 파이프라인은 최신 라인이 필요
git+https://github.com/huggingface/transformers
git+https://github.com/huggingface/diffusers
